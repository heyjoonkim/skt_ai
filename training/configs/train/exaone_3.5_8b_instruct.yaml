seed: 1234
logging_level: INFO

debug: False

tensor_parallel_size: 1

environ:
  model_cache: HF_MODEL_CACHE
  dataset_cache: HF_DATASETS_CACHE
  openai_api_key: OPENAI_API_KEY
  dummy_api_key: DUMMY_API_KEY

model:
  name: LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct
  is_api: False
  base_url: http://localhost:8000/v1

path: 
  output: /home/heyjoonkim/data/reasoning_abstention
  # output: /data1/heyjoonkim/reasoning_abstention

dataset:
  # select from : heyjoonkim/hendrycks_math, heyjoonkim/math_500
  train: heyjoonkim/hendrycks_math
  test: heyjoonkim/math_500

generation:
  template_id: 0
  num_generations: 1
  num_demonstrations: 2
  # num_demonstrations: 4
  max_new_tokens: 3000
  temperature: 0.0
  top_p: 1.0
  top_logprobs: 100


error_propagation:
  # dataset: heyjoonkim/hendrycks_math
  dataset: heyjoonkim/math_500
  # 동일한 숫자의 step만 비교하기 위해 사용
  num_steps: 5

  # trajectory 샘플링해서 생성할 때 사용
  num_generations: 30
  temperature: 0.6
  top_p: 0.95
  max_new_tokens: 800
  max_sampling_per_step: 100

  verbal_confidence_threshold: 90


  openai:
    model: o4-mini

    
training:
  # num_training_epochs: 1
  num_training_epochs: 0
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 1e-3

  test_dataset: heyjoonkim/math_500